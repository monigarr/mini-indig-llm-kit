{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "898515ef",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nAnalyze token frequency, character n-grams, and estimated morphemes\\nfrom a Kanien'kÃ©ha or other polysynthetic language corpus.\\n\\nJupyter version of ../datasets/vocab_builder.py\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7_vocab_analysis_visualizer.ipynb\n",
    "\"\"\"\n",
    "Analyze token frequency, character n-grams, and estimated morphemes\n",
    "from a Kanien'kÃ©ha or other polysynthetic language corpus.\n",
    "\n",
    "Jupyter version of ../datasets/vocab_builder.py\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d12448c7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.3-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\monig\\anaconda3\\envs\\mini-indig-llm-kit\\lib\\site-packages (2.3.1)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.2-cp313-cp313-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.59.0-cp313-cp313-win_amd64.whl.metadata (110 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.8-cp313-cp313-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\monig\\anaconda3\\envs\\mini-indig-llm-kit\\lib\\site-packages (from matplotlib) (2.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\monig\\anaconda3\\envs\\mini-indig-llm-kit\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Using cached pillow-11.3.0-cp313-cp313-win_amd64.whl.metadata (9.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\monig\\anaconda3\\envs\\mini-indig-llm-kit\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\monig\\anaconda3\\envs\\mini-indig-llm-kit\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\monig\\anaconda3\\envs\\mini-indig-llm-kit\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\monig\\anaconda3\\envs\\mini-indig-llm-kit\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.3-cp313-cp313-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 8.1/8.1 MB 44.4 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.3.2-cp313-cp313-win_amd64.whl (223 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.59.0-cp313-cp313-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 42.9 MB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.8-cp313-cp313-win_amd64.whl (71 kB)\n",
      "Downloading pillow-11.3.0-cp313-cp313-win_amd64.whl (7.0 MB)\n",
      "   ---------------------------------------- 0.0/7.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 7.0/7.0 MB 50.3 MB/s eta 0:00:00\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ---------------------------- ----------- 5/7 [contourpy]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------------- 7/7 [matplotlib]\n",
      "\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.59.0 kiwisolver-1.4.8 matplotlib-3.10.3 pillow-11.3.0 pyparsing-3.2.3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ðŸ“¦ Step 1: Install if needed\n",
    "!pip install matplotlib pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3df40951",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ§  Step 2: Import modules\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2df753ce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ“‚ Step 3: Set file paths\n",
    "input_file = \"../datasets/sample_corpus.txt\"\n",
    "output_dir = \"../datasets/vocab_analysis/\"\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f16471f5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ§¾ Step 4: Load and tokenize corpus\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = [line.strip().lower() for line in f if line.strip()]\n",
    "\n",
    "tokens = []\n",
    "for line in lines:\n",
    "    tokens.extend(re.findall(r\"\\b\\w+\\b\", line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d86772af",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ“Š Step 5: Token frequency\n",
    "token_counts = Counter(tokens)\n",
    "df_tokens = pd.DataFrame(token_counts.most_common(50), columns=[\"token\", \"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b540306c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ” Step 6: Character n-gram analysis\n",
    "char_ngrams = Counter()\n",
    "for token in tokens:\n",
    "    token = f\"_{token}_\"\n",
    "    for n in range(2, 6):\n",
    "        for i in range(len(token) - n + 1):\n",
    "            char_ngrams[token[i:i+n]] += 1\n",
    "\n",
    "df_ngrams = pd.DataFrame(char_ngrams.most_common(50), columns=[\"ngram\", \"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df147387",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ”¤ Step 7: Estimated morphemes (basic heuristic)\n",
    "morpheme_like = [t for t in tokens if \"-\" in t or len(t) > 8]\n",
    "df_morphemes = pd.DataFrame(Counter(morpheme_like).most_common(30), columns=[\"morpheme_candidate\", \"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "204ce35e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# ðŸ“ˆ Step 8: Plot results\n",
    "plt.figure(figsize=(12, 6))\n",
    "df_tokens.plot(kind=\"bar\", x=\"token\", y=\"count\", legend=False)\n",
    "plt.title(\"Top Tokens\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir + \"top_tokens.png\")\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "df_ngrams.plot(kind=\"bar\", x=\"ngram\", y=\"count\", legend=False)\n",
    "plt.title(\"Top Character N-Grams\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir + \"top_char_ngrams.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46cf22d3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Vocab analysis complete. Results saved to: ../datasets/vocab_analysis/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ðŸ“¦ Step 9: Save to CSV\n",
    "df_tokens.to_csv(output_dir + \"token_frequencies.csv\", index=False)\n",
    "df_ngrams.to_csv(output_dir + \"char_ngrams.csv\", index=False)\n",
    "df_morphemes.to_csv(output_dir + \"estimated_morphemes.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Vocab analysis complete. Results saved to:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4de56c9-4214-4601-b84d-d0f8540b4964",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
