{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898515ef",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 7_vocab_analysis_visualizer.ipynb\n",
    "\"\"\"\n",
    "Analyze token frequency, character n-grams, and estimated morphemes\n",
    "from a Kanien'kÃ©ha or other polysynthetic language corpus.\n",
    "\n",
    "Jupyter version of ../datasets/vocab_builder.py\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12448c7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ“¦ Step 1: Install if needed\n",
    "!pip install matplotlib pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df40951",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ§  Step 2: Import modules\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df753ce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ“‚ Step 3: Set file paths\n",
    "input_file = \"../datasets/sample_corpus.txt\"\n",
    "output_dir = \"../datasets/vocab_analysis/\"\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16471f5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ§¾ Step 4: Load and tokenize corpus\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = [line.strip().lower() for line in f if line.strip()]\n",
    "\n",
    "tokens = []\n",
    "for line in lines:\n",
    "    tokens.extend(re.findall(r\"\\b\\w+\\b\", line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86772af",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ“Š Step 5: Token frequency\n",
    "token_counts = Counter(tokens)\n",
    "df_tokens = pd.DataFrame(token_counts.most_common(50), columns=[\"token\", \"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b540306c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ” Step 6: Character n-gram analysis\n",
    "char_ngrams = Counter()\n",
    "for token in tokens:\n",
    "    token = f\"_{token}_\"\n",
    "    for n in range(2, 6):\n",
    "        for i in range(len(token) - n + 1):\n",
    "            char_ngrams[token[i:i+n]] += 1\n",
    "\n",
    "df_ngrams = pd.DataFrame(char_ngrams.most_common(50), columns=[\"ngram\", \"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df147387",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ”¤ Step 7: Estimated morphemes (basic heuristic)\n",
    "morpheme_like = [t for t in tokens if \"-\" in t or len(t) > 8]\n",
    "df_morphemes = pd.DataFrame(Counter(morpheme_like).most_common(30), columns=[\"morpheme_candidate\", \"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204ce35e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ“ˆ Step 8: Plot results\n",
    "plt.figure(figsize=(12, 6))\n",
    "df_tokens.plot(kind=\"bar\", x=\"token\", y=\"count\", legend=False)\n",
    "plt.title(\"Top Tokens\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir + \"top_tokens.png\")\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "df_ngrams.plot(kind=\"bar\", x=\"ngram\", y=\"count\", legend=False)\n",
    "plt.title(\"Top Character N-Grams\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir + \"top_char_ngrams.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cf22d3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ“¦ Step 9: Save to CSV\n",
    "df_tokens.to_csv(output_dir + \"token_frequencies.csv\", index=False)\n",
    "df_ngrams.to_csv(output_dir + \"char_ngrams.csv\", index=False)\n",
    "df_morphemes.to_csv(output_dir + \"estimated_morphemes.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Vocab analysis complete. Results saved to:\", output_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
