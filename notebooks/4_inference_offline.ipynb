{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f9d7e3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 4_inference_offline.ipynb\n",
    "\"\"\"\n",
    "Run the fine-tuned QLoRA model offline for local text generation or translation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b70d6b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üì¶ Step 1: Install if needed (use pre-downloaded wheels if offline)\n",
    "!pip install transformers peft bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa8c241",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üß† Step 2: Load tokenizer and model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "model_path = \"../models/llama3-8b-qlora-output\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60935cc7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üó£Ô∏è Step 3: Create generation pipeline\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c1ec6d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üß™ Step 4: Generate responses (adapt prompts to your language)\n",
    "prompt = \"Translate this sentence into Kanien‚Äôk√©ha: The water is cold.\"\n",
    "\n",
    "results = generator(prompt, max_length=128, num_return_sequences=1, do_sample=True)\n",
    "\n",
    "print(\"üó®Ô∏è Generated Response:\")\n",
    "print(results[0]['generated_text'])\n",
    "\n",
    "# Optional: Save outputs for team review\n",
    "with open(\"../models/output_log.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"PROMPT: {prompt}\\nRESPONSE: {results[0]['generated_text']}\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
