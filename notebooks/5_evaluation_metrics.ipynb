{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fc1e5c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 5_evaluation_metrics.ipynb\n",
    "\"\"\"\n",
    "Evaluation Metrics for Fine-Tuned Indigenous Language Mini-LLMs\n",
    "Focus: small-scale, culturally sensitive, low-resource evaluation techniques\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc73925",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üì¶ Step 1: Install if needed\n",
    "!pip install evaluate transformers datasets sacrebleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd54103",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üß† Step 2: Import libraries\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import evaluate\n",
    "import random\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57a70f3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üìö Step 3: Load model + tokenizer\n",
    "model_path = \"../models/llama3-8b-qlora-output\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cddeb1b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üìÑ Step 4: Load sample test dataset\n",
    "# You can swap this with your own local eval file\n",
    "test_dataset = load_dataset(\"text\", data_files=\"../datasets/sample_eval_set.txt\", split=\"train\")\n",
    "examples = test_dataset[\"text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a451351",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üß™ Step 5: Run sample generations\n",
    "def generate_response(prompt, max_len=128):\n",
    "    output = generator(prompt, max_length=max_len, num_return_sequences=1, do_sample=False)\n",
    "    return output[0]['generated_text']\n",
    "\n",
    "sample_results = []\n",
    "for i in range(min(10, len(examples))):\n",
    "    prompt = examples[i]\n",
    "    response = generate_response(prompt)\n",
    "    sample_results.append({\"prompt\": prompt, \"generated\": response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b908c2f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üìä Step 6: Automatic BLEU Score (if translations)\n",
    "bleu = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "# You must provide reference translations for this to work\n",
    "# For example only:\n",
    "references = [\n",
    "    \"The water is cold.\",  # Human reference translation\n",
    "    \"The child sings loudly.\",\n",
    "]\n",
    "predictions = [r[\"generated\"] for r in sample_results[:len(references)]]\n",
    "\n",
    "bleu_score = bleu.compute(predictions=predictions, references=[[r] for r in references])\n",
    "print(f\"üìà BLEU Score: {bleu_score['score']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3aba64",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ü™∂ Step 7: Human Evaluation Worksheet Template\n",
    "print(\"\\nüìã Human Review Template\")\n",
    "print(\"\"\"\n",
    "For each row, rate the model‚Äôs output:\n",
    "- 1 = inaccurate / unrelated\n",
    "- 2 = partially accurate but flawed\n",
    "- 3 = mostly accurate but awkward\n",
    "- 4 = accurate and understandable\n",
    "- 5 = culturally fluent and fully appropriate\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nSample Prompts and Model Outputs:\")\n",
    "for result in sample_results:\n",
    "    print(f\"\\nPROMPT: {result['prompt']}\")\n",
    "    print(f\"GENERATED: {result['generated']}\")\n",
    "    print(\"HUMAN SCORE: ____ (1-5)\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7a8b1f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üóÉÔ∏è Step 8: Save results for review\n",
    "with open(\"../models/eval_results.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for r in sample_results:\n",
    "        f.write(f\"PROMPT: {r['prompt']}\\nGENERATED: {r['generated']}\\n\\n\")\n",
    "\n",
    "print(\"‚úÖ Evaluation complete. Results saved to /models/eval_results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe97f34",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "üìå Notes for Indigenous Language Evaluation\n",
    "BLEU or ROUGE scores are not sufficient for assessing cultural or grammatical fluency.\n",
    "\n",
    "Human judgment from fluent speakers is essential.\n",
    "\n",
    "Encourage scoring on accuracy, fluency, tone, and respectfulness using a shared scale like 1‚Äì5 or emojis.\n",
    "\n",
    "Use ethics-protocols/team_worksheet_template.md to record reviewer info and language preferences.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
