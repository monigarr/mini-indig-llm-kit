{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü™∂ Kanien‚Äôk√©ha Tokenizer Demo\n",
    "\n",
    "This demo loads vocabulary rules and a sample data template for the Akwesasne dialect of Kanien‚Äôk√©ha (Mohawk), processes them into morpheme segments, and prepares data for tokenizer training and evaluation.\n",
    "\n",
    "**Author:** MoniGarr  \n",
    "**Mission:** AI Research Residency Qualification ¬∑ Onkwehonwehneha NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Install Required Packages\n",
    "!pip install pyyaml pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Import Modules\n",
    "import yaml\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÇ Load Vocab Rules (YAML)\n",
    "with open(\"kanienkeha_vocab_rules.yaml\", \"r\", encoding=\"utf-8\") as file:\n",
    "    vocab_rules = yaml.safe_load(file)\n",
    "vocab_rules.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÇ Load Sample Data Template (JSON)\n",
    "with open(\"data_template.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    sample_data = json.load(file)\n",
    "df = pd.DataFrame(sample_data)\n",
    "df[['input_text', 'translation', 'morpheme_gloss']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Build Morphemes Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÅ Flatten morphemes into high-quality JSON\n",
    "morpheme_records = []\n",
    "\n",
    "for entry in sample_data:\n",
    "    for idx, morph in enumerate(entry[\"morpheme_gloss\"]):\n",
    "        morpheme_records.append({\n",
    "            \"source_id\": entry[\"id\"],\n",
    "            \"surface_form\": morph,\n",
    "            \"token_index\": idx,\n",
    "            \"input_text\": entry[\"input_text\"],\n",
    "            \"translation\": entry[\"translation\"],\n",
    "            \"tag\": entry.get(\"morpheme_tags\", [])[idx] if idx < len(entry.get(\"morpheme_tags\", [])) else None,\n",
    "            \"stem_type\": entry.get(\"stem_type\"),\n",
    "            \"category\": entry.get(\"category\"),\n",
    "            \"usage_context\": entry.get(\"usage_context\"),\n",
    "            \"validation_status\": entry.get(\"validation_status\")\n",
    "        })\n",
    "\n",
    "with open(\"morphemes.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(morpheme_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"‚úÖ morphemes.json created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Visual Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßæ Load morphemes and inspect sample\n",
    "morphemes_df = pd.read_json(\"morphemes.json\")\n",
    "morphemes_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ó Export Hugging Face Tokenizer Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_config = {\n",
    "    \"lang\": \"kanienkeha\",\n",
    "    \"name\": \"kanienkeha-akwesasne-tokenizer\",\n",
    "    \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
    "    \"do_lower_case\": False,\n",
    "    \"model_max_length\": 128,\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"</s>\",\n",
    "    \"unk_token\": \"<unk>\",\n",
    "    \"pad_token\": \"<pad>\",\n",
    "    \"sep_token\": \"<sep>\",\n",
    "    \"cls_token\": \"<cls>\"\n",
    "}\n",
    "\n",
    "with open(\"tokenizer_config.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(tokenizer_config, f, indent=2)\n",
    "print(\"‚úÖ tokenizer_config.json generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì§ Ready for Upload\n",
    "\n",
    "You now have:\n",
    "- `morphemes.json`: Structured token data\n",
    "- `tokenizer_config.json`: HF-compatible configuration\n",
    "\n",
    "Ready for HuggingFace Hub upload or further tokenizer training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
