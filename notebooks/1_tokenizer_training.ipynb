{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa065075",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1_tokenizer_training.ipynb\n",
    "\"\"\"\n",
    "Train a custom tokenizer for a polysynthetic Indigenous language using Hugging Face Tokenizers.\n",
    "\"\"\"\n",
    "\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers, normalizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93db13fb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Load your text corpus\n",
    "corpus_path = \"../datasets/sample_corpus.txt\"\n",
    "\n",
    "with open(corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = [line.strip() for line in f if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4f1146",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Setup the tokenizer model\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.normalizer = normalizers.Sequence([\n",
    "    normalizers.NFD(),\n",
    "    normalizers.Lowercase(),\n",
    "])\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "trainer = trainers.BpeTrainer(vocab_size=4000, show_progress=True, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb66a76",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Train the tokenizer\n",
    "tokenizer.train_from_iterator(lines, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca5b756",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 4: Save the tokenizer\n",
    "tokenizer.save(\"../tokenizer/custom_tokenizer.json\")\n",
    "\n",
    "print(\"âœ… Tokenizer training complete. File saved as custom_tokenizer.json\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
