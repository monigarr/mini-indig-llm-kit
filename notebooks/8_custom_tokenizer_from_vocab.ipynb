{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dab3af9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 8_custom_tokenizer_from_vocab.ipynb\n",
    "\"\"\"\n",
    "Generate a custom tokenizer for a low-resource language (e.g., Kanien‚Äôk√©ha)\n",
    "using vocab + token frequency + YAML rule matching.\n",
    "\n",
    "Converts token frequency + vocab rules into SentencePiece tokenizer to use for LLM training\n",
    "\n",
    "Outputs: can be loaded in HF or Transformers-compatible Scripts\n",
    "    ../tokenizer/kanienkeha_tokenizer.model \n",
    "    ../tokenizer/kanienkeha_tokenizer.vocab\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e47242a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# üì¶ Step 1: Install SentencePiece\n",
    "!pip install sentencepiece pyyaml pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2146558",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# üß† Step 2: Import modules\n",
    "import sentencepiece as spm\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af99cd6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# üìÑ Step 3: Load frequency data + rules\n",
    "df = pd.read_csv(\"../datasets/vocab_analysis/token_frequencies.csv\")\n",
    "rules_path = \"../datasets/kanienkeha_vocab_rules.yaml\"\n",
    "\n",
    "with open(rules_path, \"r\") as f:\n",
    "    rules = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c834cf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# üìù Step 4: Write training input file (1 token per line, weighted)\n",
    "token_file = \"../datasets/kanienkeha_tokenizer_input.txt\"\n",
    "with open(token_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for _, row in df.iterrows():\n",
    "        for _ in range(int(row[\"count\"])):\n",
    "            f.write(row[\"token\"] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070371ba",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# üß™ Step 5: Train SentencePiece model\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=token_file,\n",
    "    model_prefix=\"../tokenizer/kanienkeha_tokenizer\",\n",
    "    vocab_size=800,\n",
    "    character_coverage=1.0,\n",
    "    model_type=\"unigram\",  # better for morphologically rich languages\n",
    "    user_defined_symbols=rules[\"prefixes\"] + rules[\"suffixes\"]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Custom tokenizer trained. Files saved to ../tokenizer/\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
