{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84def8bd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#\n",
    "# tokenizer_builder.ipynb\n",
    "# A Jupyter notebook to build and save Hugging Face compatible tokenizers from your YAML and corpus.\n",
    "#\n",
    "# Author: \n",
    "#   MoniGarr (Monica Peters), monigarr@MoniGarr.com\n",
    "#\n",
    "# This repository supports language revival & retention for\n",
    "#     Polysynthetic, Low-Resource Indigenous Languages that\n",
    "#       might lack industry standard language ISO codes.\n",
    "#\n",
    "# License: Apache 2.0\n",
    "# \n",
    "# For technical consulting, collaboration, or mentorship on Indigenous\n",
    "# Language Revival & Retention Tech Solutions (AI, XR, 3D, Cultural Protocols)\n",
    "# contact:\n",
    "#   MoniGarr (Monica Peters) â€“ monigarr@monigarr.com\n",
    "#   Founder of MoniGarr.com LLC and MohawkLanguage.ca\n",
    "#   Akwesasne-based Onkwehonwe (Indigenous, Kanienâ€™kÃ©hake, Mohawk of Akwesasne)\n",
    "#   https://www.linkedin.com/in/3dtechartist\n",
    "#\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea86471",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ðŸ““ tokenizer_builder.ipynb\n",
    "\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, normalizers\n",
    "import json\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bba2a80",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup\n",
    "corpus_path = \"../datasets/sample_corpus.txt\"\n",
    "yaml_path = \"../datasets/kanienkeha_vocab_rules.yaml\"\n",
    "save_path = \"custom_tokenizer.json\"\n",
    "vocab_size = 32000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7b20c7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ðŸ“– Load dialects and rules\n",
    "with open(yaml_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    vocab_data = yaml.safe_load(f)\n",
    "\n",
    "dialects = list(vocab_data.get(\"dialects\", {}).keys())\n",
    "print(\"ðŸ“š Dialects:\", dialects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b6a6ad",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ðŸ§ª Build tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.normalizer = normalizers.Sequence([normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()])\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=vocab_size,\n",
    "    special_tokens=[\"<unk>\", \"<pad>\", \"<s>\", \"</s>\", \"<mask>\"] +\n",
    "                   [f\"<dialect_{d}>\" for d in dialects]\n",
    ")\n",
    "\n",
    "tokenizer.train([corpus_path], trainer)\n",
    "tokenizer.save(save_path)\n",
    "print(f\"âœ… Saved tokenizer to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
