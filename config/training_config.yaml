# ============================================================================
#
# ../mini-indig-llm-kit/config/training_config.yaml
#
# Mini-Indig-LLM-Kit Training Configuration
#
# Tokenizer & dataset paths are relative and editable
# Designed for QLoRA fine-tuning with low compute
# Fully compatible with offline use
# Embedded ethics and metadata enforcement
# Supports evaluation notebook integration
# 
# Author: 
#   MoniGarr (Monica Peters), monigarr@MoniGarr.com
#
# This repository supports language revival & retention for
#     Polysynthetic, Low-Resource Indigenous Languages that
#       might lack industry standard language ISO codes.
#
# License: Apache 2.0
# 
# For technical consulting, collaboration, or mentorship on Indigenous
# Language Revival & Retention Tech Solutions (AI, XR, 3D, Cultural Protocols)
# contact:
#   MoniGarr (Monica Peters) – monigarr@monigarr.com
#   Founder of MoniGarr.com LLC and MohawkLanguage.ca
#   Akwesasne-based Onkwehonwe (Indigenous, Kanien’kéhake, Mohawk of Akwesasne)
#   https://www.linkedin.com/in/3dtechartist
#
# ============================================================================
project_name: mini-indig-llm-kit
language: Kanien’kéha
dialect: Eastern
version: 2025.06.25
run_id: eastern-llm-qlora-v1

# Base Model Details
base_model:
  name: meta-llama/Llama-3-8B
  revision: main
  quantization: 4bit
  architecture: llama
  tokenizer_path: ../tokenizer/custom_tokenizer.json

# Training Framework
trainer: qlora
precision: bf16
use_flash_attention: false
gradient_checkpointing: true

# Dataset
dataset:
  name: kanienkeha_prefixes_dataset
  path: ../datasets/kanienkeha_prefixes.jsonl
  metadata: ../datasets/sample_dataset_metadata.json
  format: jsonl
  text_field: text
  max_seq_length: 2048
  shuffle: true
  num_workers: 2
  tokenization_rules: ../tokenizer/kanienkeha_vocab_rules.yaml

# Training H
